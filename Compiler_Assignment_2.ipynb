{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3ZSRzO3U5KiVcoAb3MXzG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaoEhsanElahi/MiniLang_Parser/blob/main/Compiler_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_UjyYmy9myhO"
      },
      "outputs": [],
      "source": [
        "class Parser:\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = tokens\n",
        "        self.current_token_index = 0\n",
        "\n",
        "    def parse(self):\n",
        "        # Implement top-down parsing logic here\n",
        "        program = self.parse_program()\n",
        "        return program\n",
        "\n",
        "    def parse_program(self):\n",
        "        # Parse a sequence of statements\n",
        "        statements = []\n",
        "        while self.current_token_index < len(self.tokens):\n",
        "            statement = self.parse_statement()\n",
        "            if statement:\n",
        "                statements.append(statement)\n",
        "        return statements\n",
        "\n",
        "    def parse_statement(self):\n",
        "        # Implement logic to parse different statement types (if-else, print, assignment)\n",
        "        if self.match(TokenType.KEYWORD):\n",
        "            if self.tokens[self.current_token_index - 1].lexeme == \"if\":\n",
        "                return self.parse_if_else()\n",
        "            elif self.tokens[self.current_token_index - 1].lexeme == \"print\":\n",
        "                return self.parse_print()\n",
        "            else:\n",
        "                # Handle invalid keyword\n",
        "                pass\n",
        "        elif self.match(TokenType.IDENTIFIER):\n",
        "            # Potentially an assignment statement, needs further parsing\n",
        "            return self.parse_assignment()\n",
        "        else:\n",
        "            # Handle invalid statement start\n",
        "            pass\n",
        "\n",
        "        return None  # No statement parsed\n",
        "\n",
        "    # Implement methods for parsing if-else, print, and assignment statements\n",
        "\n",
        "    def match(self, token_type):\n",
        "        # Similar logic as scanner's match method\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "class TokenType(Enum):\n",
        "    INTEGER_LITERAL = \"INTEGER_LITERAL\"\n",
        "    BOOLEAN_LITERAL = \"BOOLEAN_LITERAL\"\n",
        "    IDENTIFIER = \"IDENTIFIER\"\n",
        "    OPERATOR = \"OPERATOR\"\n",
        "    KEYWORD = \"KEYWORD\"\n",
        "    COMMENT = \"COMMENT\"\n",
        "    WHITESPACE = \"WHITESPACE\"\n",
        "\n",
        "\n",
        "class Token:\n",
        "    def __init__(self, token_type, lexeme, line_number):\n",
        "        self.type = token_type\n",
        "        self.lexeme = lexeme\n",
        "        self.line_number = line_number\n",
        "\n",
        "\n",
        "class Scanner:\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        self.tokens = []\n",
        "        self.keywords = [\"if\", \"else\", \"print\"]\n",
        "        self.operators = [\"+\", \"-\", \"*\", \"/\", \"=\", \"==\", \"!=\"]\n",
        "        self.current_line = 1\n",
        "\n",
        "    def scan(self):\n",
        "        with open(self.filename, \"r\") as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                while line:\n",
        "                    token = self.get_next_token(line)\n",
        "                    if token:\n",
        "                        self.tokens.append(token)\n",
        "                        line = line[len(token.lexeme) :]\n",
        "                    else:\n",
        "                        print(f\"Lexical error in line {self.current_line}: Invalid token\")\n",
        "                        return\n",
        "\n",
        "                    if token.type == TokenType.COMMENT:\n",
        "                        break\n",
        "                self.current_line += 1\n",
        "\n",
        "    def get_next_token(self, line):\n",
        "        \"\"\"\n",
        "        Identifies the next token in the given line.\n",
        "        \"\"\"\n",
        "        for token_type in [TokenType.COMMENT, TokenType.WHITESPACE, TokenType.OPERATOR]:\n",
        "            match = self.match_pattern(line, TokenType.patterns[token_type])\n",
        "            if match:\n",
        "                return Token(token_type, match.group(0), self.current_line)\n",
        "\n",
        "        for token_type in [TokenType.INTEGER_LITERAL, TokenType.BOOLEAN_LITERAL, TokenType.IDENTIFIER]:\n",
        "            match = self.match_pattern(line, TokenType.patterns[token_type])\n",
        "            if match:\n",
        "                return Token(token_type, match.group(0), self.current_line)\n",
        "\n",
        "        for keyword in self.keywords:\n",
        "            if line.startswith(keyword):\n",
        "                return Token(TokenType.KEYWORD, keyword, self.current_line)\n",
        "\n",
        "        return None  # No token found\n",
        "\n",
        "    @staticmethod\n",
        "    def match_pattern(line, pattern):\n",
        "        return re.match(pattern, line)\n",
        "\n",
        "    def display_tokens(self):\n",
        "        for token in self.tokens:\n",
        "            print(f\"{token.type.name}: {token.lexeme} (line {token.line_number})\")\n",
        "\n",
        "\n",
        "TokenType.patterns = {\n",
        "    TokenType.INTEGER_LITERAL: r\"\\d+\",\n",
        "    TokenType.BOOLEAN_LITERAL: r\"true|false\",\n",
        "    TokenType.IDENTIFIER: r\"[a-zA-Z][a-zA-Z0-9]*\",\n",
        "    TokenType.OPERATOR: r\"\\+|\\-|\\*|\\/|\\=|\\=\\=|\\!\\=\",\n",
        "    TokenType.COMMENT: r\"//.*\",\n",
        "    TokenType.WHITESPACE: r\"\\s+\",\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    filename = input(\"Enter the filename to scan: \")\n",
        "    scanner = Scanner(filename)\n",
        "    scanner.scan()\n",
        "    scanner.display_tokens()\n"
      ],
      "metadata": {
        "id": "rLVbIYFzow0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3hwuK1vro9s4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}